# Good Reads on AI

## Large Language Models (LLMs)

**Stanford Alpaca**: A project that explores the fine-tuning of language models using instruction-following data.  
- Repository: [GitHub](https://github.com/tatsu-lab/stanford_alpaca)  
- Blog Post: [Stanford CRFM](https://crfm.stanford.edu/2023/03/13/alpaca.html)  

**LLMs from Scratch**: A step-by-step guide to implementing a ChatGPT-like LLM in PyTorch.  
- Repository: [GitHub](https://github.com/rasbt/LLMs-from-scratch)  

**Instruct Fine-Tuning Data**: A JSON file containing data for instruction fine-tuning.  
- Data File: [GitHub](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/instruction-data.json)  

## Research Papers

**Attention Is All You Need**: The seminal paper introducing the Transformer model, which has become foundational in NLP tasks.  
- Paper: [ArXiv](https://arxiv.org/pdf/1706.03762)  

**TinyStories: How Small Can Language Models Be and Still Speak Coherent English?**: A study exploring the minimal size requirements for language models to maintain coherence.  
- Paper: [ArXiv](https://arxiv.org/abs/2305.07759)  

**A Survey of Large Language Models**: An overview of various large language models, their architectures, and performance.  
- Paper: [ArXiv](https://arxiv.org/abs/2206.07682)  

**Evaluating Tokenizer Performance of Large Language Models Across Official Indian Languages**  
- Paper: [ArXiv](https://arxiv.org/html/2411.12240v2#:~:text=A%20lower%20proportion%20indicates%20better,a%20baseline%20tokenizer%20%5B12%5D%20.)  

**Deep Residual Learning for Image Recognition**: A breakthrough paper introducing residual networks (ResNets).  
- Paper: [ArXiv](https://arxiv.org/pdf/1508.07909)  

## Datasets

**FairytaleQA**: A dataset designed for question and answer generation tasks, focusing on fairytale narratives.  
- Repository: [GitHub](https://github.com/uci-soe/FairytaleQAData)  

**LifeArchitect AI Datasets Table**: A comprehensive table listing various datasets used in AI research.  
- Dataset Table: [LifeArchitect](https://lifearchitect.ai/datasets-table/)  

**SentencePiece**: Unsupervised text tokenizer for neural network-based text generation.  
- Repository: [GitHub](https://github.com/google/sentencepiece)  

## Tutorials and Demonstrations

**Sketch-RNN Demo**: An interactive demo showcasing a recurrent neural network that can draw sketches.  
- Demo: [Magenta](https://magenta.tensorflow.org/sketch-rnn-demo)  

**The Animated Transformer**: A visual and interactive explanation of the Transformer model architecture.  
- Tutorial: [Animated Transformer](https://prvnsmpth.github.io/animated-transformer/)  

**Word2Vec Tutorial**: A comprehensive guide on word embeddings using the Word2Vec model with TensorFlow.  
- Tutorial: [TensorFlow](https://www.tensorflow.org/text/tutorials/word2vec)  

**AutoGen Agentic Approach**: A programming framework for agentic AI.  
- Demo: [GitHub](https://github.com/microsoft/autogen)  

**Tokenizer**: Learn about language model tokenization.  
- Repository: [OpenAI](https://platform.openai.com/tokenizer)  
- Demo: [TikTokenizer](https://tiktokenizer.vercel.app/)  

**TikToken**: A fast OpenAI tokenizer implementation.  
- Repository: [GitHub](https://github.com/openai/tiktoken)

**Byte Pair Encoding**: Intro to BPE.
- Article: [Blog](https://sebastianraschka.com/blog/2025/bpe-from-scratch.html)

## Notable Projects

**Mini-R1**: An exploration into building efficient AI models with reduced computational resources.  
- Article: [Phil Schmid](https://www.philschmid.de/mini-deepseek-r1)  

**LifeArchitect AI Models Table**: A detailed comparison of various AI models, including parameters, architectures, and training details.  
- Models Table: [LifeArchitect](https://lifearchitect.ai/models-table/)  

