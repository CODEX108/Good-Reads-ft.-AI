# Good Reads on AI

## Large Language Models (LLMs)

**Stanford Alpaca**: A project that explores the fine-tuning of language models using instruction-following data.  
Repository: [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)  
Blog Post: [https://crfm.stanford.edu/2023/03/13/alpaca.html](https://crfm.stanford.edu/2023/03/13/alpaca.html)  

**LLMs from Scratch**: A step-by-step guide to implementing a ChatGPT-like LLM in PyTorch.  
Repository: [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)  

**Instruct Fine-Tuning Data**: A JSON file containing data for instruction fine-tuning.  
Data File: [https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/instruction-data.json](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/01_main-chapter-code/instruction-data.json)  

## Research Papers

**Attention Is All You Need**: The seminal paper introducing the Transformer model, which has become foundational in NLP tasks.  
Paper: [https://arxiv.org/pdf/1706.03762](https://arxiv.org/pdf/1706.03762)  

**TinyStories: How Small Can Language Models Be and Still Speak Coherent English?**: A study exploring the minimal size requirements for language models to maintain coherence.  
Paper: [https://arxiv.org/abs/2305.07759](https://arxiv.org/abs/2305.07759)  

**A Survey of Large Language Models**: An overview of various large language models, their architectures, and performance.  
Paper: [https://arxiv.org/abs/2206.07682](https://arxiv.org/abs/2206.07682)  

**Evaluating Tokenizer Performance of Large Language Models Across Official Indian Languages**
https://arxiv.org/html/2411.12240v2#:~:text=A%20lower%20proportion%20indicates%20better,a%20baseline%20tokenizer%20%5B12%5D%20.

## Datasets

**FairytaleQA**: A dataset designed for question and answer generation tasks, focusing on fairytale narratives.  
Repository: [https://github.com/uci-soe/FairytaleQAData](https://github.com/uci-soe/FairytaleQAData)  

**LifeArchitect AI Datasets Table**: A comprehensive table listing various datasets used in AI research.  
Dataset Table: [https://lifearchitect.ai/datasets-table/](https://lifearchitect.ai/datasets-table/)  

**Sentence Piece**: Unsupervised text tokenizer for Neural Network-based text generation.
Repository: https://github.com/google/sentencepie
## Tutorials and Demonstrations

**Sketch-RNN Demo**: An interactive demo showcasing a recurrent neural network that can draw sketches.  
Demo: [https://magenta.tensorflow.org/sketch-rnn-demo](https://magenta.tensorflow.org/sketch-rnn-demo)  

**The Animated Transformer**: A visual and interactive explanation of the Transformer model architecture.  
Tutorial: [https://prvnsmpth.github.io/animated-transformer/](https://prvnsmpth.github.io/animated-transformer/)  

**Word2Vec Tutorial**: A comprehensive guide on word embeddings using the Word2Vec model with TensorFlow.  
Tutorial: [https://www.tensorflow.org/text/tutorials/word2vec](https://www.tensorflow.org/text/tutorials/word2vec)  

**AutoGen Agentic Approach**: A programming framework for agentic AI.
Demo: https://github.com/microsoft/autogen

**Tokenizer**: Learn about language model tokenization.
Repository: [https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer)
Demo: [https://tiktokenizer.vercel.app/](https://tiktokenizer.vercel.app/)

**TikToken**: 
Repository : https://github.com/openai/tiktoken

## Notable Projects

**Mini-R1**: An exploration into building efficient AI models with reduced computational resources.  
Article: [https://www.philschmid.de/mini-deepseek-r1](https://www.philschmid.de/mini-deepseek-r1)  

**LifeArchitect AI Models Table**: A detailed comparison of various AI models, including parameters, architectures, and training details.  
Models Table: [https://lifearchitect.ai/models-table/](https://lifearchitect.ai/models-table/)  

